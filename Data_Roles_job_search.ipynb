{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1V0T5+aI0iCs1L/dyZr6Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SebastienBienfait/L2C-Data-managment/blob/main/Data_Roles_job_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Job Search Script for  and JobsInKent.   \n",
        "\n",
        "###Intro:\n",
        "This script accepts user input to search the websites, Reed.co.uk and jobsinkent.com. The user can specify a search term, center of search and radius of search. The user can then specify a filter term which will only return jobs with that term included job title.   \n",
        "\n",
        "If users wish to save results of the search as a .csv file they can do so in their Google Drive.   \n",
        "\n",
        "Further down in the script, the data is split into two catagories. Jobs that are posted by a recruitment agency and jobs that are posted by direct employers. An initial list of know recruites is taken from GitHub but it is not expanist*. The list can be added to by direct user imput if the decide any in the list of direct employers are actually recruiters. The updated list can then be saved to Google Drive and used in subsequent searches using this script."
      ],
      "metadata": {
        "id": "gXmOBWlBvpJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing libaries and defining base jobsite urls"
      ],
      "metadata": {
        "id": "bU4maESh2aQ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j-UR0WPmrjU2"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None  # default='warn'  ##<- this is really annoying, I WANT to copy the DF.\n",
        "import numpy as np\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Z3FeetaZyGf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###User Input Function:    \n",
        "To use this programe, input your search terms in the box below. \n",
        "\n",
        "Job Search Term: You can include commers, spaces and multiple words.\n",
        "\n",
        "Center of Search, use format: 'Town, County' if the center you are searching for is not unique. If it is unique within the UK, 'County' is unnecessary. Or you can just use 'County' as your search center.\n",
        "\n",
        "--NB: for JobsInKent, the search is limited to 'Kent' by default.\n",
        "\n",
        "Radius: enter any valid integer. The closest valid radius for each site will be picked. If you are equidistance between two values, the lower value will be choesen.\n",
        "\n",
        "Filter Term: This will filter the final database to only contain jobs with the Filiter Term in the title: ie 'Data' will filter keep 'Data Developer' but not 'Software Developer'. To not filter at all, type 'no' instead.\n",
        "\n",
        "--NB: the filter term is case sensitve, 'data' will not pick up 'Database'.\n",
        "\n",
        "Specify a folder to save in: To save the DataFrame in the folder MyDrive/Collab Notebooks/Folder 1, enter 'Collab Notebooks/Folder 1' when asked."
      ],
      "metadata": {
        "id": "ho_4zEqKkdX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def user_imput():\n",
        "  search_term = input(\"Job Search Term (commer, spaces are allowed): \" )\n",
        "  search_center = input(\"Center of Search (Town, County): \") #town, county\n",
        "  search_radius = int(input(\"Search Radius (Miles): \" ))\n",
        "\n",
        "  filter_term = input(\"Filter Term: \")\n",
        "  if filter_term.lower() == \"no\":\n",
        "    filter_term = None\n",
        "\n",
        "  save_state = False\n",
        "  query_save = input(\"Would you like to save and download the results of your search? (yes/no) \")\n",
        "  if query_save.lower() == \"yes\":\n",
        "    save_state = True\n",
        "    save_path = input(\"Specify a folder in My Drive to save in (or leave blank): \")\n",
        "  else: \n",
        "    save_state = False\n",
        "    save_path = \"\"\n",
        "\n",
        "  recruiters_list_test = input(\"Do you have a list of known recruiters saved in your Drive? (yes/no): \")\n",
        "\n",
        "  return search_term, search_center, search_radius, filter_term, save_path, recruiters_list_test\n",
        "\n",
        "search_term, search_center, search_radius, filter_term, save_path, recruiters_list_test = user_imput()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "Hg-Ws572kcQv",
        "outputId": "386271e5-7323-4472-c81a-e751a7610813"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Job Search Term (commer, spaces are allowed): Data\n",
            "Center of Search (Town, County): Ashford, Kent\n",
            "Search Radius (Miles): 30\n",
            "Filter Term: Data\n",
            "Would you like to save and download the results of your search? (yes/no) yes\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ee28e73ae9f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msearch_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_radius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecruiters_list_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0msearch_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_radius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecruiters_list_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_imput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-ee28e73ae9f2>\u001b[0m in \u001b[0;36muser_imput\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mquery_save\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"yes\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msave_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Specify a folder in My Drive to save in (or leave blank): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msave_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function finds the closest (rounded down) search radius to the on submitted."
      ],
      "metadata": {
        "id": "FKwEcOgRCLnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_valid_radius(radius,valid_list):\n",
        "  valid_test = np.array([],dtype=\"int\")\n",
        "\n",
        "  for radi in valid_list:\n",
        "    valid_test = np.append(valid_test,[abs(radi-radius)])\n",
        "  return_radius = valid_list[np.argmin(valid_test)]\n",
        "  return return_radius"
      ],
      "metadata": {
        "id": "tQ-qy5zUCLC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This takes the search terms above and creates the URL"
      ],
      "metadata": {
        "id": "La4s0FpEa-Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def creat_urls(search_term, search_center, search_radius):\n",
        "  jobsinkent_search_str = search_term.replace(\",\",\"%2C\").replace(\" \",\"+\")\n",
        "  reed_search_str = search_term.replace(\",\",\"%2C\").replace(\" \",\"%20\")\n",
        "\n",
        "  reed_center_str = search_center.replace(\", \",\"%2C-\")\n",
        "\n",
        "  jobsinkent_valid_radius = np.array([1,2,5,10,20,30,40],dtype=\"int\")\n",
        "  jobsinkent_radius = find_valid_radius(search_radius, jobsinkent_valid_radius)\n",
        "\n",
        "  reed_valid_radius = np.array([0,1,3,5,10,15,20,30,50],dtype=\"int\")\n",
        "  reed_radius = find_valid_radius(search_radius, reed_valid_radius)\n",
        "\n",
        "  jobsinkent_url = \"https://jobsinkent.com/search?q=\" + jobsinkent_search_str + \"&pl=1\" + \"&r=\" + str(jobsinkent_radius)\n",
        "  reed_url = \"https://www.reed.co.uk/jobs?keywords=\" + reed_search_str+\"&location=\" + reed_center_str + \"&proximity=\" + str(reed_radius)\n",
        "  print(jobsinkent_url)\n",
        "  print(reed_url)\n",
        "\n",
        "  return reed_url, jobsinkent_url"
      ],
      "metadata": {
        "id": "cMh4CqyMCnpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reed_url, jobsinkent_url = creat_urls(search_term, search_center, search_radius)\n",
        "\n",
        "max_job_call = 2000 #reed has a limit of 2000 job search api calls per hour."
      ],
      "metadata": {
        "id": "sk5LBD_iCKXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "General Functions that are used in both searches."
      ],
      "metadata": {
        "id": "j9izuNBqxuTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#returns the full HTML of a page.\n",
        "def get_html(url):\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "#This cleans the dataframes when they have been created.\n",
        "def general_clean(df,site,filter_term):\n",
        "  df.drop_duplicates(inplace=True)\n",
        "  df[\"salary\"].replace(np.nan, '', regex=False,inplace=True) #not all companies have listed a salary and so NaN is sometimes returned\n",
        "  df[\"company\"] = df[\"company\"].str.lstrip(\" \").str.rstrip(\" \") #some companies have excess whitespaces around their names\n",
        "  df.dropna(inplace=True) #just incase the max of 2000 jobs is bypassed\n",
        "\n",
        "  if filter_term != None:\n",
        "    df = df[df[\"job_title\"].str.contains(filter_term)]\n",
        "\n",
        "  df[\"date_found\"] = datetime.date.today()\n",
        "  df[\"advertised_on\"] = site\n",
        "  return df"
      ],
      "metadata": {
        "id": "hPZ9tXTmxuHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reed Job Search\n",
        "---"
      ],
      "metadata": {
        "id": "twZFKrtmuBG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function finds the job ID for all jobs displayed on each page url from reed.co.uk\n"
      ],
      "metadata": {
        "id": "Ze1w318xtMpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reed_jobs_iterate(page_url):\n",
        "  site_html = get_html(page_url)\n",
        "  results = site_html.find(class_=\"col-sm-8 col-md-9 results-container\")\n",
        "\n",
        "  id_list = np.array([])\n",
        "  job_cards = results.find_all(class_=\"job-result-card\")\n",
        "\n",
        "  for job in job_cards: #finds the jobs, gets it's ID, returns the id\n",
        "      job_id = int(job[\"id\"].split(\"jobSection\")[1]) #jobSection48529572 ect...\n",
        "      \n",
        "      id_list = np.append(id_list,[job_id])\n",
        "  return id_list"
      ],
      "metadata": {
        "id": "3HhCS-c4rov7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This finds the total jobs found in the search"
      ],
      "metadata": {
        "id": "FcBpN7SV7sI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_reed_total_jobs(site_html,max_job_call):\n",
        "  max_page_requests = int(max_job_call/25) \n",
        "\n",
        "  ###\n",
        "  total_jobs_text = site_html.find(class_=\"col-sm-11 col-xs-12 page-title\").text # '\\n' '\\r' '\\n' x,xxx\\r\\n  Data Jobs near Ashford       '\\n'....\n",
        "  total_jobs_text = total_jobs_text.replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\",\",\"\") #          xxxx         Data Jobs near Ashford       .       \n",
        "\n",
        "  total_jobs = int(total_jobs_text.split(\"Data\")[0].strip(\" \"))\n",
        "  total_pages = int(np.ceil(total_jobs/25))\n",
        "\n",
        "  #limits the requests to only 2000\n",
        "  if total_pages > max_page_requests:\n",
        "    total_pages = max_page_requests\n",
        "\n",
        "\n",
        "  return total_jobs, total_pages"
      ],
      "metadata": {
        "id": "rb1-9VOLuMXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The function scrapes website for the job IDs of every job that has been found by the search criteria.\n"
      ],
      "metadata": {
        "id": "55hjm4Hqtadz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reed_scrape(base_url,max_job_call):\n",
        "  soup = get_html(base_url)\n",
        "  total_id_list = np.array([],dtype=int)\n",
        "\n",
        "  #finds out how many more pages to scrape\n",
        "  total_jobs, total_pages = find_reed_total_jobs(soup,max_job_call)\n",
        "\n",
        "  print(\"total jobs found: \", total_jobs)\n",
        "  print(\"total pages: \", total_pages)\n",
        "  print(\"pages to search: \",total_pages)\n",
        "\n",
        "  for page_no in np.arange(1,total_pages+1): \n",
        "    page_url = base_url+\"&pageno=\"+str(page_no)\n",
        "    page_id_list = reed_jobs_iterate(page_url)\n",
        "    total_id_list = np.append(total_id_list,[page_id_list])\n",
        "\n",
        "    #user feedback\n",
        "    print(\"page \",page_no, \" processed\")\n",
        "  return total_id_list\n"
      ],
      "metadata": {
        "id": "Az2WHxc4taFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives the job IDs to the Reed API to return job information based on the job ID"
      ],
      "metadata": {
        "id": "vTIXRwtw7FDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def call_reed_api(id_list):\n",
        "  base_url = \"https://www.reed.co.uk/api/1.0/jobs/\"\n",
        "  api_url = \"\"\n",
        "  job_df = pd.DataFrame()\n",
        "\n",
        "  #user feedback\n",
        "  print(\"Retriving job information...\")\n",
        "\n",
        "  for job_id in id_list:\n",
        "    api_url = base_url+str(job_id)\n",
        "    post_request = requests.get(api_url, auth=(\"96a7ec49-549c-4529-b9d2-fa3059a437b3\",\"\"))\n",
        "    json_data = post_request.json()\n",
        "\n",
        "    new_row = pd.json_normalize(json_data)\n",
        "    job_df = job_df.append(new_row,ignore_index=True)\n",
        "  return job_df"
      ],
      "metadata": {
        "id": "XvS9gJzw3naG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calls the scraping function and the api function in turn ~7-8mins to run"
      ],
      "metadata": {
        "id": "KMBzMmFP8FKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reed_id_list = reed_scrape(reed_url,max_job_call)\n",
        "reed_id_list = reed_id_list.astype(\"int\") #it really didnt want to save the array as an int\n",
        "\n",
        "full_reed_df = call_reed_api(reed_id_list)\n",
        "display(full_reed_df.info())"
      ],
      "metadata": {
        "id": "9LUfycOh7OMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning the Reed DF"
      ],
      "metadata": {
        "id": "q8H8F0IT-Z40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_reed(df_in): #this filters out the necessary rows and renames them so the cleaning function works.\n",
        "  df = df_in[[\"jobTitle\", \"employerName\", \"salary\", \"contractType\", \"locationName\",  \"jobUrl\"]]\n",
        "\n",
        "  df.rename(columns={\"jobTitle\":\"job_title\", \"employerName\": \"company\", \"contractType\":\"contract\", \"locationName\":\"location\",  \"jobUrl\":\"job_url\"}, inplace=True)\n",
        "  return df\n",
        "\n",
        "reed_df = clean_reed(full_reed_df)\n",
        "reed_df = general_clean(reed_df,\"https://www.reed.co.uk\",filter_term)\n",
        "display(reed_df.shape)\n",
        "display(reed_df.head(5))"
      ],
      "metadata": {
        "id": "_Sp0u-sv-YlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#JobsInKent Job Search\n",
        "---\n"
      ],
      "metadata": {
        "id": "TS2GBSrFtbB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function returns a DF containing all of the relevent jobs found on 1 page"
      ],
      "metadata": {
        "id": "REZ6sHSk9apg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jobsinkent_iterate(url,job_limit):\n",
        "  jobs_df = pd.DataFrame(data=[], columns = [\"job_title\", \"company\", \"salary\", \"contract\", \"location\",\"job_url\"])\n",
        "  site_html = get_html(url)\n",
        "  results = site_html.find(class_=\"mt-2 lg:mt-0\")\n",
        "\n",
        "  #this is finding an additional 20 jobs per page that are not visible on the site itself.\n",
        "  job_cards = results.find_all(class_=\"flex mb-2 border-black-900 rounded-md border-x-4 sm:rounded-lg\")\n",
        "\n",
        "\n",
        "  #[:job_limit] is required as non relevent jobs were being found in the site_html with the job_cards class after the 20~ jobs that were being show on the site.\n",
        "  # So I had to find how many jobs were being show in each page first and then limit the loop to only iterate up to that number of jobs.\n",
        "  for job in job_cards[:job_limit]:\n",
        "\n",
        "    job_title_url = job.find(\"a\", class_=\"text-blue-700 visited:text-grey-200 hover:underline\") \n",
        "    job_title = job_title_url.text\n",
        "    job_url = job_title_url[\"href\"]\n",
        "\n",
        "    company_name = job.find(class_=\"mt-0 mb-2 max-w-2xl text-sm text-black\").text \n",
        "\n",
        "    #This info was all on the fouth box class and needed to be seperated further for the relevent information to be extracted.\n",
        "    info_list = job.find_all(class_=\"col-span-12\") \n",
        "    salary = clean_bloat(info_list[0].text)\n",
        "    contract = clean_bloat(info_list[1].text)\n",
        "    location = clean_bloat(info_list[2].text)\n",
        "\n",
        "    new_row = {\"job_title\":job_title, \"company\":company_name, \"salary\":salary, \"contract\":contract, \"location\":location,\"job_url\":job_url}\n",
        "    jobs_df = jobs_df.append(new_row,ignore_index = True)\n",
        "  return jobs_df"
      ],
      "metadata": {
        "id": "9gfyVlh-r4TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "String manipulation funtions."
      ],
      "metadata": {
        "id": "qy4TyxFjDcgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#removes '\\n' and spaces from from names.\n",
        "def clean_bloat(text):\n",
        "  strip1 = text.replace(\"\\n\",\"\").replace(\"\\xa0\",\"\")\n",
        "  strip2 = strip1.strip(\" \")\n",
        "  return strip2\n",
        "\n",
        "#extracts information about numbers of jobs \n",
        "def return_jobs_pages(input_str):\n",
        "  p_j = input_str.split(\"of\") #[showing 1 to yy] , [xx jobs]\n",
        "  jobs_per_page = int(p_j[0].split(\"to\")[1]) #[yy]\n",
        "  total_jobs = int(p_j[1].split(\"Jobs\")[0]) #[xx]\n",
        "  \n",
        "  total_pages = int(np.ceil(total_jobs/jobs_per_page))\n",
        "\n",
        "  return total_jobs, jobs_per_page, total_pages"
      ],
      "metadata": {
        "id": "sUWfIcbWI3SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main function that calls to find out how many pages are needed to search over and stores the results of each iteration call."
      ],
      "metadata": {
        "id": "SOPwBYdi-a6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jobsinkent_scrape(base_url):\n",
        "  #initialse the DF that will be used\n",
        "  jobsinkent_df = pd.DataFrame(data=[], columns = [\"job_title\", \"company\", \"salary\", \"contract\", \"location\",\"job_url\"])\n",
        "\n",
        "  #returns ALL the html from the site\n",
        "  site_html = get_html(base_url)\n",
        "\n",
        "  banner = site_html.find(\"p\", class_=\"text-sm text-gray-700 leading-5\") #~6th child within the top banner.\n",
        "  showing = banner.text # 'Showing 1 to yy of xx Jobs'\n",
        "\n",
        "  total_jobs, jobs_per_page, total_pages = return_jobs_pages(showing)\n",
        "\n",
        "  for page_no in range(1,total_pages+1):\n",
        "    #this if statment is required due to the issues mentioned above about job limits. It sets the jobs_limit variable to however many jobs there are displayed (visibly) on the page.\n",
        "    if page_no == total_pages:\n",
        "      jobs_limit = total_jobs%jobs_per_page\n",
        "    else:\n",
        "      jobs_limit = jobs_per_page\n",
        "\n",
        "    page_url = base_url + \"&page=\" + str(page_no)\n",
        "    new_df = jobsinkent_iterate(page_url, jobs_limit)\n",
        "    jobsinkent_df = jobsinkent_df.append(new_df, ignore_index=True)\n",
        "  \n",
        "    #user feedback  \n",
        "    print(\"page \",page_no, \" processed\")\n",
        "\n",
        "  return jobsinkent_df\n",
        "  \n",
        "jobsinkent_df = jobsinkent_scrape(jobsinkent_url)\n",
        "display(jobsinkent_df.head(5))"
      ],
      "metadata": {
        "id": "Vsag8HN2JKKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleans the jobsinkent_df to be used later."
      ],
      "metadata": {
        "id": "c47hzOfRJku6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jobsinkent_df = general_clean(jobsinkent_df,\"jobsinkent.com\",filter_term)"
      ],
      "metadata": {
        "id": "4QbSbRcSJlEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Combining both results\n",
        "---\n"
      ],
      "metadata": {
        "id": "kW11z72pKmed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combines the two DF's from JobsInKent"
      ],
      "metadata": {
        "id": "47bVltbM1Kqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_jobs_df = pd.DataFrame()\n",
        "\n",
        "data_jobs_df = data_jobs_df.append([jobsinkent_df,reed_df],ignore_index=True)\n",
        "display(data_jobs_df.head())"
      ],
      "metadata": {
        "id": "A1jAk3srKp42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def importing_recruiters(recruiters_list_test,save_folder):\n",
        "  \n",
        "  if recruiters_list_test == \"yes\":\n",
        "    drive.mount('/content/drive')\n",
        "    list_of_recruiters = pd.read_csv(\"/content/drive/MyDrive/\" + save_folder + \"/recruiters_list.csv\")\n",
        "    display(list_of_recruiters)\n",
        "    drive.flush_and_unmount\n",
        "  else:\n",
        "    list_of_recruiters = pd.read_csv(\"https://raw.githubusercontent.com/futureCodersSE/data-roles/main/datasets/recruiters_list.csv\")\n",
        "\n",
        "  return list_of_recruiters\n",
        "\n",
        "list_of_recruiters = importing_recruiters(recruiters_list_test,save_folder)\n"
      ],
      "metadata": {
        "id": "2jrsKgNDlXvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This returns a **unique** list of recruiters and compaies who are advertiseing data roles"
      ],
      "metadata": {
        "id": "A_Ke9HwHMJ0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_company_lists1(initail_df):\n",
        "  recruiters_list = list_of_recruiters[\"company\"]\n",
        "  data_jobs_employer_names = initail_df.drop_duplicates(subset = \"company\")\n",
        "\n",
        "  companies_df = data_jobs_employer_names.drop( data_jobs_employer_names[data_jobs_employer_names[\"company\"].isin(recruiters_list) == True ].index)\n",
        "  companies_list = companies_df[\"company\"]\n",
        "\n",
        "  return companies_list, recruiters_list\n",
        "\n",
        "companies_list, recruiters_list = find_company_lists1(data_jobs_df)\n"
      ],
      "metadata": {
        "id": "VPA44suO9dmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes the list of known Recruitment Agencies from GitHub and splits the DataFrame and return the list of know Recruitment Agencies and Companies that are in that DataFrame"
      ],
      "metadata": {
        "id": "Y7eVdVqC978d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_company_lists():\n",
        "  data_jobs_employer_names = data_jobs_df.drop_duplicates(subset = \"company\") #data_jobs_df[\"company\"].unique()\n",
        "\n",
        "  #seperating out all recruitment companies from the list\n",
        "  companies_df = data_jobs_employer_names.drop( data_jobs_employer_names[ data_jobs_employer_names[\"company\"].str.contains(\"Recruit\")==True].index)\n",
        "\n",
        "  #This requires going through each company and deciding if they are a recruitment agency or not if they dont have 'Recruit' in their name and adding them to the list.\n",
        "  additional_recruiter_list = [\"Reed\",\"OnetoOne Personnel\",\"Huntress\",\"MW Appointments\",\"P3 Search & Selection\",\"Manpower - Ashford\",\"Morgan McKinley\",\"Brook Street\",\"McGregor Boyall\",\"SAGA\",\"Office Angels\",\"REED\",\n",
        "                            \"Commercial Services Interim & Executive Search\",\"Harnham\",\"Connect2Staff\",\"Senitor Associates\",\"Zorba Consulting Limited\",\"GerrardWhite\",\"Lorien\",\"Morgan Law\", \"Academics\",\"eFinancialCareers\",\n",
        "                            \"Commercial Services HR\", \"OBJECTIVE HR LTD\" ]\n",
        "  it_specialits = [\"Senitor Associates\",\"Zorba Consulting Limited\",\"GerrardWhite\",\"Lorien\",\"eFinancialCareers\"]\n",
        "\n",
        "  companies_df = companies_df.drop( companies_df[companies_df[\"company\"].isin(additional_recruiter_list) == True ].index)\n",
        "  companies_list = companies_df[\"company\"]\n",
        "\n",
        "  recruiters_df1 = data_jobs_employer_names.drop( data_jobs_employer_names[ data_jobs_employer_names[\"company\"].str.contains(\"Recruit\")==False].index) #drop 'Recruit' is not in name\n",
        "  recruiters_df2 = data_jobs_employer_names.drop( data_jobs_employer_names[data_jobs_employer_names[\"company\"].isin(additional_recruiter_list) == False ].index) #drop if company is not in 'additional_drop_list'\n",
        "\n",
        "  recruiters_df = recruiters_df1.append(recruiters_df2,ignore_index=True) #combine the two sets of lists\n",
        "  recruiters_list = recruiters_df[\"company\"]\n",
        "\n",
        "  return companies_list, recruiters_list\n",
        "\n",
        "companies_list, recruiters_list = find_company_lists()"
      ],
      "metadata": {
        "id": "BFpCe9WMMKEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function prints the list of Companies for the user to manually check if they are Recruitment Agencies. By entering in the index of any suspected recruitment agancies, these are dropped from companies list and added to the recruiters list.   \n",
        "\n",
        "User Input: \"2,14,20...\" or \"2, 14, 20...\""
      ],
      "metadata": {
        "id": "UNAwTH46s3MU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "companies_list = companies_list.reset_index()[\"company\"]\n",
        "\n",
        "full_recruiters_series = pd.Series([])\n",
        "\n",
        "def find_more_recruiters(companies_list, recruiters_found):\n",
        "  all_checked = False\n",
        "  n = 0 #counter\n",
        "  while all_checked != True:\n",
        "    list_max = len(companies_list)\n",
        "    print(\"companies to check: \",list_max)\n",
        "\n",
        "    print(n)\n",
        "    while n+20 <= list_max:\n",
        "\n",
        "      for i in range(n,n+20):   #company in companies_list[n:n+5]:\n",
        "        print(i, companies_list[i])\n",
        "\n",
        "      recruiter_index_str = input(\"Enter the Index of a Recruitment Agencies on this list or type 'no' to contiune \")\n",
        "\n",
        "      if recruiter_index_str.lower() == \"no\":\n",
        "        n += 20\n",
        "      else:\n",
        "        recruiter_index_list = recruiter_index_str.replace(\" \",\"\").split(\",\")\n",
        "        recruiter_index_ints = [eval(x) for x in recruiter_index_list]\n",
        "\n",
        "        drop_index_S = pd.Series( [ companies_list.loc[recruiter_index_ints] ] )\n",
        "        print(drop_index_S[0])\n",
        "\n",
        "        recruiters_found = recruiters_found.append(drop_index_S[0], ignore_index=True)\n",
        "        companies_list.drop(recruiter_index_ints, inplace = True)\n",
        "        companies_list = companies_list.reset_index()[\"company\"]\n",
        "        list_max -= len(drop_index_S[0])\n",
        "      ###########\n",
        "      #End of Inner While Statment.\n",
        "      ###########\n",
        "\n",
        "    for i in range(n,list_max):\n",
        "      print(i, companies_list[i])\n",
        "\n",
        "    recruiter_index_str = input(\"Enter the Index of a Recruitment Agencies on this list or type 'no' to contiune \")\n",
        "\n",
        "    if recruiter_index_str == \"no\":\n",
        "      all_checked = True\n",
        "      print(\"All Companies Have Been Checked!\")\n",
        "      print(\"droped companies: \")\n",
        "      display(recruiters_found)\n",
        "      ###########\n",
        "      #End of Outer While Statment.\n",
        "      ###########\n",
        "    else:\n",
        "        recruiter_index_list = recruiter_index_str.replace(\" \",\"\").split(\",\")\n",
        "        recruiter_index_ints = [eval(x) for x in recruiter_index_list]\n",
        "\n",
        "        drop_index_S = pd.Series( [ companies_list.loc[recruiter_index_ints] ] )\n",
        "        print(drop_index_S[0])\n",
        "\n",
        "        recruiters_found = recruiters_found.append(drop_index_S[0], ignore_index=True)\n",
        "        companies_list.drop(recruiter_index_ints, inplace = True)\n",
        "        companies_list = companies_list.reset_index()[\"company\"]\n",
        "        list_max -= len(drop_index_S[0])\n",
        "\n",
        "\n",
        "  return companies_list, recruiters_found\n",
        "\n",
        "\n",
        "companies_list, recruiters_found = find_more_recruiters(companies_list, full_recruiters_series)\n",
        "\n"
      ],
      "metadata": {
        "id": "LTxKu7rRgVOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This funtion combines the list of known Recruiment Agencies and the list found by the user. To update the .CSV file on GitHub. Download the file and upload it to https://github.com/futureCodersSE/data-roles/tree/main/datasets under the same name. You can rerun the previouse few cells to check that the updated list is effective."
      ],
      "metadata": {
        "id": "QbdwrMcA_hkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_recruiters_list(recruiters_list, recruiters_found):\n",
        "  recruiters_list = recruiters_list.append(recruiters_found,ignore_index=True)\n",
        "  display(recruiters_list)\n",
        "\n",
        "  drive.mount('/content/drive')\n",
        "  recruiters_list.to_csv(\"recruiters_list.csv\",index = 0)\n",
        "update_recruiters_list(recruiters_list, recruiters_found)"
      ],
      "metadata": {
        "id": "OYRHyX7YlFiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This takes the lists of unique compaies and recruiters and returns a DataFrame with **all** the roles they have advertised."
      ],
      "metadata": {
        "id": "NFXbvgXzWkoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_company_df():\n",
        "  company_df = data_jobs_df.drop( data_jobs_df[data_jobs_df[\"company\"].isin(recruiters_list) == True ].index) #drop if company is a recruiter\n",
        "  recruiters_df = data_jobs_df.drop( data_jobs_df[data_jobs_df[\"company\"].isin(companies_list) == True ].index) #drop if company is not a recruiter\n",
        "\n",
        "  #For readability.\n",
        "  company_df.reset_index(drop=True, inplace = True)\n",
        "  recruiters_df.reset_index(drop=True, inplace = True)\n",
        "\n",
        "  return company_df, recruiters_df\n",
        "\n",
        "company_df, recruiters_df = find_company_df()\n",
        "  "
      ],
      "metadata": {
        "id": "QlYMcg9iNht-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saves the DataFrames with a name based on the date searched. If you run the search multiple times, the newer search file will overwrite the older search file."
      ],
      "metadata": {
        "id": "TT9h2KnWU7kJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#saveing files names of search based on current date.\n",
        "date_today = datetime.date.today()\n",
        "recruiters_fn = str(date_today)+\"_recruitment_company_df.csv\"\n",
        "company_fn = str(date_today)+\"_data_employeer_df.csv\"\n",
        "\n",
        "#'opens' the chosen folder in drive for files to be saved in.\n",
        "def mount_drive(data_path):\n",
        "  drive.mount('/content/drive/', force_remount=True)\n",
        "  project_dir = \"/content/drive/MyDrive/\" + data_path #--working parth\n",
        "  return project_dir\n",
        "\n",
        "  \n",
        "def unmount_drive():\n",
        "  drive.flush_and_unmount()\n",
        "  print('Drive Unmounted')\n",
        "\n",
        "project_dir = mount_drive(save_path)"
      ],
      "metadata": {
        "id": "_4JzYHo4PXAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks if you have choesn to save your work to your Drive."
      ],
      "metadata": {
        "id": "DRsunvMr2cbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if query_save.lower() == \"yes\":\n",
        "    save_state = True\n",
        "    \n",
        "    recruiters_list.to_csv(project_dir +\"/recruiters_list.csv\")\n",
        "    recruiters_df.to_csv(project_dir + recruiters_fn)\n",
        "    company_df.to_csv(project_dir + company_fn)\n",
        "\n",
        "    unmount_drive()\n",
        "  else: \n",
        "    save_state = False"
      ],
      "metadata": {
        "id": "y1db4ipQeRlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prints confirmation of successful save and the file name/location."
      ],
      "metadata": {
        "id": "dlE1Z443L9iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if save_state == True:\n",
        "  save()\n",
        "  download()\n",
        "  print(\"Saved and Printed\")\n",
        "else:\n",
        "  print(\"You did not save the DF\")\n",
        "\n",
        "print(\"Search Terms:\")\n",
        "print(search_term)\n",
        "print(search_center)\n",
        "print(search_radius)"
      ],
      "metadata": {
        "id": "QhCI_tWI2cRG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}